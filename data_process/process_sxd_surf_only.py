import json
import os
import argparse
from glob import glob
from tqdm import tqdm
import pickle
import numpy as np
import torch
import nvdiffrast.torch as dr

import scipy
import skimage
import einops

import random

import igl
from geomdl import fitting, BSpline, utilities

from concurrent.futures import ThreadPoolExecutor, as_completed
from multiprocessing import Pool

from matplotlib.colors import to_rgba

from geometry_utils.obj import read_obj  # type: ignore

torch.set_grad_enabled(False)

_CMAP = {
    "帽": {"alias": "帽", "color": "#F7815D"},
    "领": {"alias": "领", "color": "#F9D26D"},
    "肩": {"alias": "肩", "color": "#F23434"},
    "袖片": {"alias": "袖片", "color": "#C4DBBE"},
    "袖口": {"alias": "袖口", "color": "#F0EDA8"},
    "衣身前中": {"alias": "衣身前中", "color": "#8CA740"},
    "衣身后中": {"alias": "衣身后中", "color": "#4087A7"},
    "衣身侧": {"alias": "衣身侧", "color": "#DF7D7E"},
    "底摆": {"alias": "底摆", "color": "#DACBBD"},
    "腰头": {"alias": "腰头", "color": "#DABDD1"},
    "裙前中": {"alias": "裙前中", "color": "#46B974"},
    "裙后中": {"alias": "裙后中", "color": "#6B68F5"},
    "裙侧": {"alias": "裙侧", "color": "#D37F50"},

    "橡筋": {"alias": "橡筋", "color": "#696969"},
    "木耳边": {"alias": "木耳边", "color": "#696969"},
    "袖笼拼条": {"alias": "袖笼拼条", "color": "#696969"},
    "荷叶边": {"alias": "荷叶边", "color": "#696969"},
    "绑带": {"alias": "绑带", "color": "#696969"}
}

_PANEL_CLS = [
    '帽', '领', '肩', '袖片', '袖口', '衣身前中', '衣身后中', '衣身侧', '底摆', '腰头', '裙前中', '裙后中', '裙侧', '橡筋', '木耳边', '袖笼拼条', '荷叶边', '绑带']

_PANEL_COLORS = np.array(
    [(0., 0., 0., 0.)] + [to_rgba(_CMAP[_PANEL_CLS[idx]]['color'])
                          for idx in range(len(_PANEL_CLS))]
)

_AVATAR_BBOX = np.array([
    [-449.00006104,  191.10876465, -178.34872437],      # min
    [447.45980835, 1831.29016113,  174.13575745]       # max
])

_GLOBAL_SCALE = 2000.0
_GLOBAL_OFFSET = np.array([0.0, 1000.0, 0.0])

_GLOBAL_SCALE_UV = 1500.0
_GLOBAL_OFFSET_UV = np.array([0.0, 1000.0])


def _get_bbox(point_sets, points_mask=None):
    num_sets = point_sets.shape[0]
    p_dim = point_sets.shape[-1]
    
    if points_mask is None: points_mask = np.ones_like(point_sets[..., :1]).astype(bool)
    
    bboxes = []
    for idx in range(num_sets):
        valid_pnts = point_sets[idx].reshape(-1, p_dim)[points_mask[idx].reshape(-1)]
        min_point, max_point = np.min(valid_pnts, axis=0), np.max(valid_pnts, axis=0)
        bboxes.append(np.concatenate([min_point, max_point]))
    
    return np.vstack(bboxes)


def _get_scale_offset(point_sets, points_mask):
    p_dim = point_sets.shape[-1]
    valid_pnts = point_sets.reshape(-1, p_dim)[points_mask.reshape(-1)]

    min_vals = np.min(valid_pnts, axis=0)
    max_vals = np.max(valid_pnts, axis=0)
    
    offset = min_vals + (max_vals - min_vals) / 2
    scale = max(max_vals - min_vals)
    
    assert scale != 0, 'scale is zero'
    
    return offset, scale


def _interpolate_feature_dr(rast, pos, tris, feat, antialias=False):

    out, _ = dr.interpolate(feat[None], rast, tris)
    if antialias: out = dr.antialias(out, rast, pos, tris)
    out = out.detach().cpu().numpy()

    ############## visualize sample points ##############
    # out = out[:, ::-1, :, :]     # flip image coordinate
    # for idx in range(out.shape[0]):
    #     plt.imshow(out[idx])
    #     plt.show()
    #     plt.imshow(out[idx])
    #     plt.show()

    return out


def _sample_curve_points_2d(edge_spec, num_samples=32):
    bezierPts = np.asarray(edge_spec['bezierPoints'])[:, :2]
    ctrlPts = np.asarray(edge_spec['controlPoints'])[:, :2]

    if np.any(bezierPts) and len(ctrlPts) == 2:
        if not np.any(bezierPts[1]):
            # print('quadratic bezier')
            bezierPts[1] = 2.0 / 3.0 * \
                (bezierPts[0] + ctrlPts[0] - ctrlPts[1])
            bezierPts[0] = 2.0 / 3.0 * bezierPts[0]

        bezierPts = ctrlPts + bezierPts
        curve = BSpline.Curve()
        curve.degree = 3
        curve.ctrlpts = [ctrlPts[0].tolist()] + \
            bezierPts.tolist() + [ctrlPts[1].tolist()]
        curve.knotvector = utilities.generate_knot_vector(
            curve.degree, len(curve.ctrlpts))
        curve.sample_size = num_samples
        curve.evaluate()

        evalpts = np.array(curve.evalpts)

    else:
        if ctrlPts.shape[0] <= 2:
            evalpts = ctrlPts
            t_values = np.linspace(0, 1, num_samples)
            evalpts = np.outer(
                1 - t_values, evalpts[0]) + np.outer(t_values, evalpts[1])
        else:
            curve = fitting.interpolate_curve(
                ctrlPts.tolist(), degree=2 if ctrlPts.shape[0] < 5 else 3)
            curve.sample_size = num_samples
            curve.evaluate()
            evalpts = np.array(curve.evalpts)

    return evalpts


def _project_curve_to_line_segments(curve_pts, line_segments, line_features):
    """
    Project curve points to the boundary edges of a triangle mesh, returen its 3d positions.

    Parameters:
    - curve_pts: query points, A numpy array of shape (num_points, 2)
    - line_segments: line segments a numpy array of shape (num_edges, 2, 2)
    - line_features: line features a numpy array of shape (num_edges, 2, feature_dim)

    Returns:
    - An array of shape (num_points, feature_dim), where each elements is the projected feature of the query point.
    """

    # find the belonging line segment
    query_dist_start = np.linalg.norm(
        curve_pts[:, None, :] - line_segments[:, 0, :][None], axis=-1)       # (num_sample, num_edges)
    query_dist_end = np.linalg.norm(
        curve_pts[:, None, :] - line_segments[:, 1, :][None], axis=-1)       # (num_sample, num_edges)
    query_dist = query_dist_start + query_dist_end

    belonging_segments = np.argmin(query_dist, axis=-1)

    start_feat = line_features[belonging_segments, 0, :]
    end_feat = line_features[belonging_segments, 1, :]

    dist_start = query_dist_start[np.arange(
        query_dist_start.shape[0]), belonging_segments]
    dist_end = query_dist_end[np.arange(
        query_dist_end.shape[0]), belonging_segments]
    dist_total = query_dist[np.arange(query_dist.shape[0]), belonging_segments]

    assert np.allclose(dist_total, dist_start +
                       dist_end), "Invalid distance calculation!"

    t_start = dist_start / dist_total
    t_end = dist_end / dist_total

    query_feat = t_start[:, None] * start_feat + t_end[:, None] * end_feat

    return belonging_segments, query_feat


def _get_vert_cls(mesh_obj, 
                  pattern_spec, 
                  cls_label={'bg': 0, 'surf': 1, 'edge': 2, 'vert': 3}):
    
    verts = mesh_obj.points
    uv = mesh_obj.point_data['obj:vt']
    normals = mesh_obj.point_data['obj:vn']
    
    panel_data = dict([(x['id'], x) for x in pattern_spec['panels']])
    
    # by default all points are surface points, set vert class to '1'
    vert_cls = np.ones_like(verts[:, 0], dtype=np.uint8) * cls_label['surf']    
    
    for idx, panel_id in enumerate(mesh_obj.field_data['obj:group_tags']):
        panel_faces = mesh_obj.cells[idx].data
        panel_spec = panel_data[panel_id]
        
        panel_center = np.asarray(panel_spec['center'])
        
        # identifying edge vertices
        boundary = igl.boundary_facets(panel_faces)
        boundary_verts = np.unique(boundary)
        vert_cls[boundary_verts] = cls_label['edge']
        
        # identifying corner vertices
        panel_corner_uvs = np.concatenate(
            [np.asarray(seg_edge['vertices'], dtype=np.float32) for seg_edge in panel_spec['seqEdges']],
            axis=0
        )
        boundary_uv = uv[boundary_verts, :] - panel_center[None]
        panel_corner_vert_dists = np.linalg.norm(
            panel_corner_uvs[:, None, :] - boundary_uv[None], axis=-1)
        panel_corner_vert_idx = np.argmin(panel_corner_vert_dists, axis=-1)
        vert_cls[panel_corner_vert_idx] = cls_label['vert']
                       
    mesh_obj.point_data['obj:cls'] = vert_cls
                       
    return vert_cls 
    
    
def _downsample_sparse_pooling(x, factor=16, anti_aliasing=False):
    """ downsample input array with sparse pooling
    Args:
        x: np.ndarray, (H, W, C + 1), original feature tensor of shape (H, W, C) with additional occupancy channel
        factor: int, downsample factor
        anti_aliasing: bool, whether to use anti_aliasing
    Returns:
        'feat_down': np.ndarray, downsampled feat with edge snapping
    """
    # assuming feat is 1024 x 1024
    occ = x[..., -1]>=.5
    eroded_mask = scipy.ndimage.binary_erosion(occ, structure=np.ones((3,3))) # sqaure strucure is needed to get the corners
    edge_occ = ~eroded_mask & occ
    edge_feat = x.copy()
    edge_feat[edge_occ==0] = -1.
    
    edge_occ_patches = einops.rearrange(edge_occ, '(h1 h2) (w1 w2) -> h1 w1 h2 w2', h2=factor, w2=factor)
    edge_occ_down = edge_occ_patches.max(axis=-1).max(axis=-1)
    eod_0_count  = (edge_occ_patches==0).sum(axis=-1).sum(axis=-1)
    eod_1_count  = (edge_occ_patches==1).sum(axis=-1).sum(axis=-1)
    edge_feat_patches = einops.rearrange(edge_feat, '(h1 h2) (w1 w2) c-> h1 w1 h2 w2 c', h2=factor, w2=factor)
    edge_feat_down = edge_feat_patches.sum(axis=-2).sum(axis=-2) + eod_0_count[...,None]
    edge_feat_down = np.divide(edge_feat_down, eod_1_count[...,None], out=np.zeros_like(edge_feat_down), where=eod_1_count[...,None]!=0)

    downsampled = skimage.transform.resize(
        x, (x.shape[0]//factor,)*2, order=0, 
        preserve_range=False, anti_aliasing=anti_aliasing)
    
    # edge snapping
    downsampled = edge_feat_down * (edge_occ_down[...,None]) + downsampled * (1-edge_occ_down[...,None])
        
    return downsampled
    
    
def prepare_edge_data(
    mesh_obj,
    pattern_spec,
    reso=64
):

    verts = mesh_obj.points
    uv = mesh_obj.point_data['obj:vt']
    normals = mesh_obj.point_data['obj:vn']

    panel_data = dict([(x['id'], x) for x in pattern_spec['panels']])
    edge_ids, edge_wcs, edge_uvs = [], [], []
    corner_wcs, corner_uvs, corner_normals = [], [], []
    
    faceEdge_adj = {}

    for idx, panel_id in enumerate(mesh_obj.field_data['obj:group_tags']):
        if panel_id not in faceEdge_adj: faceEdge_adj[panel_id] = []
        panel_faces = mesh_obj.cells[idx].data
        panel_spec = panel_data[panel_id]

        panel_center = np.asarray(panel_spec['center'])

        boundary = igl.boundary_facets(panel_faces)
        boundary_uv = np.stack([
            (uv[boundary[:, 0], :]-panel_center)[:, :2],
            (uv[boundary[:, 1], :]-panel_center)[:, :2]],
            axis=1
        )
        boundary_pos = np.stack(
            [verts[boundary[:, 0], :], verts[boundary[:, 1], :]], axis=1)
        boundary_normals = np.stack(
            [normals[boundary[:, 0], :], normals[boundary[:, 1], :]], axis=1)

        panel_corner_uvs = []

        for seg_edge in panel_spec['seqEdges']:
            
            # skip inner line, only consider sew line
            if seg_edge['type'] != 3: continue
            panel_corner_uvs.append(np.asarray(seg_edge['vertices'], dtype=np.float32))

            # processing edge
            for edge in seg_edge['edges']:
                edge_samples = _sample_curve_points_2d(edge, reso)

                edge_bbox = (np.min(edge_samples, axis=0)-5.0,
                             np.max(edge_samples, axis=0)+5.0)

                # find boundary vertices within the edge bounding box
                within_bbox = np.all(
                    (boundary_uv[:, 0, :] >= edge_bbox[0]) &
                    (boundary_uv[:, 0, :] <= edge_bbox[1]) &
                    (boundary_uv[:, 1, :] >= edge_bbox[0]) &
                    (boundary_uv[:, 1, :] <= edge_bbox[1]), axis=-1)

                if within_bbox.sum() == 0: continue
                
                _, sample_pos = _project_curve_to_line_segments(
                    edge_samples, boundary_uv[within_bbox, :, :], 
                    boundary_pos[within_bbox, :, :]
                    )
                
                edge_samples = edge_samples + panel_center[None, :2]

                edge_ids.append(edge['id'])
                edge_wcs.append(sample_pos)
                edge_uvs.append(edge_samples)

                faceEdge_adj[panel_id].append(edge['id'])

        # processing corner
        panel_corner_uvs = np.concatenate(panel_corner_uvs, axis=0)
        boundary_verts = np.unique(boundary)
        boundary_uv = uv[boundary_verts, :] - panel_center[None]
        boundary_pos = verts[boundary_verts, :]
        boundary_normals = normals[boundary_verts, :]

        panel_corner_vert_dists = np.linalg.norm(
            panel_corner_uvs[:, None, :] - boundary_uv[None], axis=-1)
        panel_corner_vert_idx = np.argmin(panel_corner_vert_dists, axis=-1)
        panel_corner_wcs = boundary_pos[panel_corner_vert_idx, :]
        panel_corner_normals = boundary_normals[panel_corner_vert_idx, :]
        panel_corner_uvs = (panel_corner_uvs + panel_center[None])[..., :2]
        
        # global verts coordinates
        corner_wcs.append(panel_corner_wcs)
        corner_uvs.append(panel_corner_uvs)
        corner_normals.append(panel_corner_normals)

    edge_wcs = np.stack(edge_wcs, axis=0)
    edge_uvs = np.stack(edge_uvs, axis=0)
    
    corner_uvs = np.concatenate(corner_uvs, axis=0)
    corner_wcs = np.concatenate(corner_wcs, axis=0)
    corner_normals = np.concatenate(corner_normals, axis=0)

    return edge_ids, edge_wcs, edge_uvs, corner_wcs, corner_uvs, corner_normals, faceEdge_adj


def prepare_surf_data(
    glctx,
    mesh_obj,
    pattern_spec,
    reso=64          # original rasterization resolution
):

    verts = torch.from_numpy(mesh_obj.points).to(torch.float32).to('cuda')
    uv = torch.from_numpy(mesh_obj.point_data['obj:vt']).to(
        torch.float32).to('cuda')
    normals = torch.from_numpy(mesh_obj.point_data['obj:vn']).to(torch.float32).to('cuda')

    panel_data = dict([(x['id'], x) for x in pattern_spec['panels']])

    panel_ids = []
    panel_cls = []
    
    uv_local = uv.clone()               # projected pixel coordinate for each vertex

    tris = []                           # all triangles
    tri_ranges = []                     # triangle range for each panel,
    start_idx = 0
    
    for idx, panel_id in enumerate(mesh_obj.field_data['obj:group_tags']):
        panel_ids.append(panel_id)

        panel_seg_id = panel_data[panel_id]['label'].strip()
        panel_seg_id = _PANEL_CLS.index(panel_seg_id) + 1 if panel_seg_id in _PANEL_CLS else 0.
        panel_cls.append(panel_seg_id)

        panel_faces = mesh_obj.cells[idx].data
        vert_ids = np.unique(panel_faces)

        tris.append(torch.from_numpy(panel_faces).to('cuda'))
        tri_ranges.append([start_idx, len(panel_faces)])
        start_idx += len(panel_faces)

        panel_bbox2d = torch.cat([
            torch.min(uv_local[vert_ids, :], dim=0, keepdim=True)[0],
            torch.max(uv_local[vert_ids, :], dim=0, keepdim=True)[0]
        ])

        uv_local[vert_ids, :] = (uv_local[vert_ids, :] - panel_bbox2d[0]) / (
            panel_bbox2d[1] - panel_bbox2d[0] + 1e-6)      # normalize to [0, 1]
        
        
    tris = torch.cat(tris, dim=0).to(torch.int32)
    # panel triangle range#
    tri_ranges = torch.tensor(tri_ranges, dtype=torch.int32)

    # normalize to [-1, 1]
    uv_local = uv_local[..., :2] * 2.0 - 1.0
    uv_local = torch.cat([
        uv_local,
        torch.zeros_like(uv_local[:, :1]),
        torch.ones_like(uv_local[:, :1])], dim=1)

    rast, _ = dr.rasterize(
        glctx, uv_local, tris, resolution=[1024, 1024], 
        ranges=tri_ranges, grad_db=False)

    vert_feat = torch.cat([verts, uv[..., :2], normals], dim=-1)    # dim: (N, 8), xyz + uv + normal
    surf_feat = _interpolate_feature_dr(rast, uv_local, tris, vert_feat)
    surf_mask = (rast[..., 3:]>0).squeeze(0).cpu().numpy()
        
    down_factor = 1024 // reso
    if down_factor > 1:
        downsampled_feat = []
        for s_idx in range(surf_feat.shape[0]):
            downsampled_feat.append(
                _downsample_sparse_pooling(
                    np.concatenate([surf_feat[s_idx], surf_mask[s_idx]], axis=-1), 
                    factor=down_factor, anti_aliasing=False
                    )
                )
        
        surf_feat = np.stack(downsampled_feat, axis=0)
        surf_feat, surf_mask = surf_feat[..., :-1], surf_feat[..., -1:].astype(bool)
        
    # spliting surf_feat
    surf_pnts, surf_uvs, surf_norms = \
        surf_feat[..., :3], surf_feat[..., 3:5], surf_feat[..., 5:8]
                                    
    return panel_ids, panel_cls, surf_pnts, surf_uvs, surf_norms, surf_mask


def normalize(
    surf_pnts,
    surf_masks=None,
    global_offset=None, global_scale=None):
    
    """
    Various levels of normalization 
    """
            
    # Global normalization to -1~1
    if global_offset is None or global_scale is None:
        global_offset, global_scale = _get_scale_offset(surf_pnts, surf_masks)
        assert global_scale != 0, 'scale is zero'

    surfs_wcs, surfs_ncs = [], []

    # Normalize surface
    if surf_masks is None: surf_masks = np.ones_like(surf_pnts[..., :1]).astype(bool)
    # print('[NORMALIZE] surf_mask: ', surf_pnts.shape, surf_masks.shape, np.unique(surf_masks))  
    for surf_idx in range(surf_pnts.shape[0]):
        surf_pnt, surf_mask = surf_pnts[surf_idx], surf_masks[surf_idx]
        surf_pnt_wcs = (
            surf_pnt - global_offset[np.newaxis, np.newaxis, :]) / (global_scale * 0.5)
        surf_pnt_wcs = surf_pnt_wcs * surf_mask.astype(surf_pnt_wcs.dtype)
        surfs_wcs.append(surf_pnt_wcs)
        
        local_offset, local_scale = _get_scale_offset(surf_pnt_wcs, surf_mask)
        pnt_ncs = (surf_pnt_wcs - local_offset[np.newaxis, np.newaxis, :]) / (local_scale * 0.5)
        pnt_ncs = pnt_ncs * surf_mask.astype(pnt_ncs.dtype)
        surfs_ncs.append(pnt_ncs)

    surfs_wcs = np.stack(surfs_wcs)
    surfs_ncs = np.stack(surfs_ncs)

    return surfs_wcs, surfs_ncs, global_offset, global_scale


def process_data(
        data_item: str,
        glctx=None,
        num_surf_samples=256):

    # print('*** Processing data: ', data_item)

    mesh_fp = os.path.join(data_item, os.path.basename(data_item)+'.obj')
    pattern_fp = os.path.join(data_item, 'pattern.json')

    mesh_obj = read_obj(mesh_fp)
    with open(pattern_fp, 'r', encoding='utf-8') as f:
        pattern_spec = json.load(f)

    # surf_uv: panel 3D points [num_panels, num_samples, num_samples, 3]
    if glctx is None: glctx = dr.RasterizeCudaContext()

    _, _, surf_pnts, _, surf_norms, surf_mask = prepare_surf_data(
        glctx, mesh_obj=mesh_obj, pattern_spec=pattern_spec, reso=num_surf_samples
    )
    
    surf_pnts = (surf_pnts - _GLOBAL_OFFSET[np.newaxis, np.newaxis, :]) / (_GLOBAL_SCALE * 0.5)
    print('*** surf data: ', surf_pnts.shape, surf_norms.shape, surf_mask.shape)    
    result = np.concatenate([surf_pnts, surf_mask, surf_norms], axis=-1)        
    
    return result


def process_item(data_idx, data_item, args, glctx):
    try:
        os.makedirs(args.output, exist_ok=True)
        output_fp = os.path.join(args.output, '%05d.pkl' % (data_idx))    
        
        result = process_data(
            data_item, glctx=glctx, 
            num_surf_samples=args.nf)
        
        with open(output_fp, 'wb') as f: pickle.dump(result, f)
        return True, data_item
        
    except Exception as e:
        return False, f"{data_item} | [ERROR] {e}"


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="Draw panel bbox 3D")
    parser.add_argument("-i", "--input", default="./resources/examples",
                        type=str, help="Input directories splited bt comma.")
    parser.add_argument("-o", "--output", default='./resources/examples/processed',
                        type=str, help="Output directory.")
    parser.add_argument("-r", "--range", default=None, type=str, 
                        help="Path to executable.")
    parser.add_argument("--use_uni_norm", action='store_true',
                        help="Whether to apply universal normalization for the output data. Use predefined global scale and offset for the whole dataset if specified otherwise calculate from each data item.")
    parser.add_argument('--nf', default=256, type=int, help='Number of surface samples.')

    args, cfg_cmd = parser.parse_known_args()

    glctx = dr.RasterizeCudaContext()
    print('[DONE] Init renderer.', type(glctx))

    data_root_dirs = args.input.split(',')
    print('Input directories: ', data_root_dirs)
    
    data_items = []
    for idx, data_root in enumerate(data_root_dirs):
        cur_data_items = sorted([os.path.dirname(x) for x in glob(
            os.path.join(data_root, '**', 'pattern.json'), recursive=True)])
        data_items += cur_data_items
        print('[%02d/%02d] Found %d items in %s.'%(idx+1, len(data_root_dirs), len(cur_data_items), data_root))
    print('Total items: ', len(data_items))

    log_file = os.path.join(args.output, 'app.log')
    if os.path.exists(log_file):
        with open(log_file, "r") as f:
            lines = f.readlines()
            processed = [x.split("\t")[0] for x in lines if x.split("\t")[1].strip() == "1"]
            data_items = [x for x in data_items if x not in processed]

    if args.range is not None:
        if ',' in args.range:
            begin, end = args.range.split(",")
            begin, end = max(0, int(begin)), min(int(end), len(data_items))
            data_items = data_items[begin:end]
            print("Extracting range: %d %s" % (len(data_items), args.output))
        else:
            data_items = random.choices(data_items, k=int(args.range))
            print("Extracting random items: %d %s" % (len(data_items), args.output))

    os.makedirs(args.output, exist_ok=True)

    failed_items = []
    with open(log_file, 'a+') as f:
        with ThreadPoolExecutor(max_workers=1) as executor:  # 可以调整max_workers以改变并行度
            futures = {executor.submit(
                process_item, data_idx, data_item, args, glctx): data_item for data_idx, data_item in enumerate(data_items)}

            for future in tqdm(as_completed(futures), total=len(futures)):
                result, data_item = future.result()
                if not result:
                    data_item, err_code = data_item.split('|')
                    data_item = data_item.strip()
                    err_code = err_code.strip()
                    failed_items.append(data_item)
                    print('[ERROR] Failed to process data:', data_item, err_code)
                    f.write(f"{data_item}\t 0\n")
                else:
                    f.write(f"{data_item}\t 1\n")
                    
    with open(os.path.join(args.output, 'failed_items.log'), 'w') as f:
        for item in failed_items: f.write(item+'\n')